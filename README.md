In my research, I focus on the core challenges of Large Language Models (LLMs), specifically through the lens of scalability, hallucination, and optimization. I apply my skills to build more robust and efficient models, using techniques such as quantization and knowledge distillation to improve model performance and deployment. My work on Retrieval-Augmented Generation (RAG) helps reduce hallucinations by grounding model outputs in verified data sources, ensuring factual accuracy. This approach, combined with my experience in fine-tuning models like LoRA, allows me to create highly optimized and trustworthy AI systems.

[x dot com](https://x.com/CllTheCoder) ⋅ [kaggle](https://www.kaggle.com/carloscll) ⋅ [cv](https://drive.google.com/file/d/1rOAT7LlJDYMnf0Beh9Xpa7VKlH0ivbT_/view?usp=sharing) ⋅ [cv of failures](https://drive.google.com/file/d/1rUWMwI5iZlwFymLMd24yHOgO_NVNaEEH/view?usp=sharing) ⋅ [blog](https://cllspy.github.io/CllTorch-Blog/)  ⋅  [huggingface](https://huggingface.co/CASLL)
